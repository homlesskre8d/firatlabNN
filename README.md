# firatlabNN
первая лабораторная вар 7
Введение
В данной лабораторной работе рассматривается процесс обучения сверточной нейронной сети SqueezeNet с использованием оптимизатора AmsGrad. SqueezeNet — это легковесная архитектура сверточных нейронных сетей, разработанная для эффективного использования вычислительных ресурсов при сохранении высокой точности классификации изображений. Основная идея SqueezeNet заключается в уменьшении количества параметров модели за счет использования специальных блоков "fire modules", которые сочетают в себе операции сжатия (squeeze) и расширения (expand).

Теоретическая база
SqueezeNet
SqueezeNet — это архитектура, которая позволяет достичь сопоставимой с AlexNet точности, но с использованием в 50 раз меньшего количества параметров. Это достигается за счет:

Fire Modules: Блоки, состоящие из двух слоев: слоя сжатия (squeeze) с малым количеством фильтров (1x1 свертки) и слоя расширения (expand) с комбинацией 1x1 и 3x3 сверток.

Уменьшения количества параметров: За счет использования 1x1 сверток и стратегического уменьшения количества фильтров.

Позднего уменьшения разрешения: Уменьшение разрешения изображения происходит на более поздних этапах сети, что позволяет сохранить больше информации.

Оптимизатор AmsGrad
AmsGrad — это модификация алгоритма оптимизации Adam, которая была предложена для решения проблемы неоптимальной сходимости в некоторых случаях. Основные особенности AmsGrad:

Исправление проблемы затухания learning rate: В отличие от Adam, AmsGrad не позволяет learning rate уменьшаться слишком быстро, что может привести к более стабильной сходимости.

Использование максимального значения градиента: AmsGrad использует максимальное значение исторических градиентов для обновления параметров, что помогает избежать излишнего уменьшения learning rate.

Применение AmsGrad в обучении нейронных сетей
AmsGrad особенно полезен в задачах, где требуется высокая точность и стабильность обучения, например, в задачах классификации изображений. Его использование позволяет улучшить сходимость модели и избежать проблем, связанных с затуханием learning rate.

Цель работы
Целью данной лабораторной работы является реализация и обучение модели SqueezeNet с использованием оптимизатора AmsGrad на наборе данных изображений. В процессе работы будут рассмотрены следующие этапы:

Подготовка данных.

Реализация архитектуры SqueezeNet.

Настройка оптимизатора AmsGrad.

Обучение модели и анализ результатов.

Описание кода
FireModule: Реализация блока FireModule, который состоит из слоя сжатия (squeeze) и слоя расширения (expand).

SqueezeNet: Архитектура SqueezeNet, которая использует несколько блоков FireModule для уменьшения количества параметров.

CarModelsDataset: Класс для загрузки и обработки данных из набора изображений автомобилей.

train_test: Функция для обучения и тестирования модели.

Основной блок: Загрузка данных, инициализация модели, настройка оптимизатора и запуск обучения.

Обучение SqueezeNet с AmsGrad
'''
Эпоха 1, Время обучения: 69.67c., Потери: 5.2786, Точность: 0.64%
Эпоха 2, Время обучения: 41.86c., Потери: 5.2781, Точность: 0.83%
Эпоха 3, Время обучения: 46.32c., Потери: 5.2777, Точность: 0.83%
Эпоха 4, Время обучения: 44.53c., Потери: 5.2776, Точность: 0.83%
Эпоха 5, Время обучения: 45.21c., Потери: 5.2773, Точность: 0.83%
Эпоха 6, Время обучения: 45.38c., Потери: 5.2772, Точность: 0.83%
Эпоха 7, Время обучения: 45.88c., Потери: 5.2771, Точность: 0.83%
Эпоха 8, Время обучения: 45.74c., Потери: 5.2770, Точность: 0.83%
Эпоха 9, Время обучения: 45.81c., Потери: 5.2769, Точность: 0.83%
Эпоха 10, Время обучения: 46.15c., Потери: 5.2768, Точность: 0.83%
Тестовая точность: 0.85%
'''
Обучение SqueezeNet с Adam
'''
Эпоха 1, Время обучения: 75.9c., Потери: 5.2789, Точность: 0.36%
Эпоха 2, Время обучения: 40.93c., Потери: 5.2782, Точность: 0.76%
Эпоха 3, Время обучения: 42.08c., Потери: 5.2778, Точность: 0.83%
Эпоха 4, Время обучения: 46.18c., Потери: 5.2774, Точность: 0.83%
Эпоха 5, Время обучения: 46.3c., Потери: 5.2773, Точность: 0.83%
Эпоха 6, Время обучения: 52.31c., Потери: 5.2771, Точность: 0.83%
Эпоха 7, Время обучения: 49.83c., Потери: 5.2771, Точность: 0.83%
Эпоха 8, Время обучения: 47.0c., Потери: 5.2769, Точность: 0.83%
Эпоха 9, Время обучения: 44.16c., Потери: 5.2769, Точность: 0.83%
Эпоха 10, Время обучения: 43.65c., Потери: 5.2768, Точность: 0.83%
Тестовая точность: 0.85%
'''
Adam показал себя слегка лучше
